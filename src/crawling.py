import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv


def find_crawling(find_value):
    # https://github.com/yashwanth2804/TickerSymbol
    # Using tickersearch API By Yashwanth2804
    url = "https://ticker-2e1ica8b9.now.sh//keyword/" + find_value
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    data = response.json()
    result = ""
    for item in data:
        result += (f"{item['name']}\n"
                   f"Ticker: {item['symbol']}\n"
                   f"\n")

    return result


def search_crawling(search_value):
    # Set the headers to mimic a web browser
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    url = "https://www.google.com/search?q=" + search_value
    # Send a GET request to the Google search URL with the query parameter
    response = requests.get(url, headers=headers)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Extract the HTML content from the response
        html_content = response.text

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(html_content, "html.parser")

        # Find all the search result elements
        search_results = soup.find_all("div", class_="yuRUbf")

        # Extract the search result titles and URLs
        message = ""
        for result in search_results:
            title = result.find("h3").text
            result_url = result.find("a")["href"]
            message += f"{title}\n{result_url}\n\n"

        return message
    else:
        return None


def kbo_crawling():
    url = "https://www.koreabaseball.com/TeamRank/TeamRank.aspx"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
    }

    # ì£¼ì–´ì§„ URLì— GET ìš”ì²­ ë³´ë‚´ê¸°
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")

    table = soup.find("table")
    table_rows = table.find_all("tr")[1:]  # Skip the header row

    team_data_list = []

    for row in table_rows:
        columns = row.find_all("td")
        rank = columns[0].get_text()
        team_name = columns[1].get_text()
        games_played = columns[2].get_text()
        wins = columns[3].get_text()
        losses = columns[4].get_text()
        draws = columns[5].get_text()
        winning_percentage = columns[6].get_text()
        win_streak = columns[7].get_text()
        recent_wins = columns[9].get_text()

        team_data = {
            "Rank": rank,
            "Team Name": team_name,
            "Games Played": games_played,
            "Wins": wins,
            "Losses": losses,
            "Draws": draws,
            "Winning Percentage": winning_percentage,
            "Win Streak": win_streak,
            "Recent Wins": recent_wins
        }

        team_data_list.append(team_data)

    return team_data_list


def kbo_now_crawling():
    # https://velog.io/@hjw4287/BeautifulSoup%EB%A5%BC-%ED%86%B5%ED%95%9C-%EB%84%A4%EC%9D%B4%EB%B2%84-%EC%8A%A4%ED%8F%AC%EC%B8%A0-%ED%81%AC%EB%A1%A4%EB%A7%81
    # ìœ„ ë¸”ë¡œê·¸ ì°¸ê³ í•¨

    # êµ­ë‚´ ì•¼êµ¬
    url = "https://sports.news.naver.com/kbaseball/schedule/index"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36"
    }
    res = requests.get(url, headers=headers)
    res.raise_for_status()

    soup = BeautifulSoup(res.text, 'lxml')
    # naverì—ì„œ divë¥¼ ë‚ ì§œë³„ í™€ìˆ˜ì™€ ì§ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ë†“ìŒ
    soup_data = [soup.find_all("div", {"class": "sch_tb"}), soup.find_all("div", {"class": "sch_tb2"})]
    datalist = []
    for data_table in soup_data:
        for data in data_table:
            # ì¼ì
            date_value = data.find("span", {"class": "td_date"}).text
            # ì •ë ¬ì„ ìœ„í•œ ì¼ì í¬ë§· í†µì¼
            if len(date_value.split(" ")[0].split(".")[1]) == 1:
                date_value = date_value.split(" ")[0].split(".")[0] + ".0" + date_value.split(" ")[0].split(".")[
                    1] + " " + date_value.split(" ")[1]
            match_cnt = data.find("td")["rowspan"]
            for i in range(int(match_cnt)):
                match_data = {"date": date_value,
                              "time": data.find_all("tr")[i].find("span", {"class": "td_hour"}).text}
                # í•´ë‹¹ ì¼ìì— ê²½ê¸°ê°€ ì—†ì„ ì‹œ naver sportsì—ì„œ match_data["time"] = "-" ë°˜í™˜
                if match_data["time"] != "-":
                    # í™ˆíŒ€
                    match_data["home"] = data.find_all("tr")[i].find("span", {"class": "team_rgt"}).text
                    # ì–´ì›¨ì´íŒ€
                    match_data["away"] = data.find_all("tr")[i].find("span", {"class": "team_lft"}).text
                    # VSì¼ ì‹œ ì˜ˆì • ê²½ê¸°
                    if data.find_all("tr")[i].find("strong", {"class": "td_score"}).text != "VS":
                        # í™ˆíŒ€ ì ìˆ˜
                        match_data["home_score"] = \
                            data.find_all("tr")[i].find("strong", {"class": "td_score"}).text.split(":")[1]
                        # ì–´ì›¨ì´íŒ€ ì ìˆ˜
                        match_data["away_score"] = \
                            data.find_all("tr")[i].find("strong", {"class": "td_score"}).text.split(":")[0]
                    else:
                        match_data["home_score"] = "-"
                        match_data["away_score"] = "-"
                    # ì¤‘ê³„
                    match_data["broadcast"] = data.find_all("tr")[i].find_all("span", {"class": "td_stadium"})[
                        0].text.strip().replace("\n", "").replace("\t", "")
                    # ê²½ê¸°ì¥
                    match_data["stadium"] = data.find_all("tr")[i].find_all("span", {"class": "td_stadium"})[1].text
                else:
                    match_data["home"] = "-"
                    match_data["away"] = "-"
                    match_data["home_score"] = "-"
                    match_data["away_score"] = "-"
                    match_data["broadcast"] = "-"
                    match_data["stadium"] = "-"
                datalist.append(match_data)
    results = sorted(datalist, key=lambda k: (k["date"], k["time"]))
    return results


def weather_crawling(nx, ny):
    # https://github.com/az0t0/discord-seoultechbot/blob/main/src/weather.py
    # ìœ„ repository ì°¸ê³ í•¨
    load_dotenv()
    token = os.environ.get('WEATHER_API_TOKEN')

    today = datetime.now()
    url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getUltraSrtFcst'
    direction = ['ë¶', 'ë¶ë¶ë™', 'ë¶ë™', 'ë™ë¶ë™', 'ë™', 'ë™ë‚¨ë™', 'ë‚¨ë™', 'ë‚¨ë‚¨ë™', 'ë‚¨',
                 'ë‚¨ë‚¨ì„œ', 'ë‚¨ì„œ', 'ì„œë‚¨ì„œ', 'ì„œ', 'ì„œë¶ì„œ', 'ë¶ì„œ', 'ë¶ë¶ì„œ', 'ë¶']

    basetime = today
    if int(basetime.strftime('%H%M')[2:4]) < 45:
        basetime = today - timedelta(hours=1)

    params = {'serviceKey': token, 'dataType': 'JSON', 'numOfRows': '1000', 'base_date': basetime.strftime('%Y%m%d'),
              'base_time': basetime.strftime('%H') + '30', 'nx': nx, 'ny': ny}

    response = requests.get(url, params=params, timeout=20)
    items = response.json().get('response').get('body').get('items')

    data = [[], [], [], [], [], []]

    for i, item in enumerate(items['item']):
        if i < 6:
            # Kor. to Eng.
            data[i] = ['temperature', 'status_num', 'status', 'rain_type', 'precipitation', 'humidity', 'wind_vane', 'wind_direction', 'wind_speed', 'time']
            if int(item['fcstTime'][0:2]) < 10:
                data[i % 6][9] = item['fcstTime'][1]
            else:
                data[i % 6][9] = item['fcstTime'][0:2]

        # ê°•ìˆ˜ í˜•íƒœ (PTY)
        # Add few more options
        elif 6 <= i < 12:
            data[i % 6][3] = item['fcstValue']
            if item['fcstValue'] == '0':
                continue
            elif item['fcstValue'] == '1':
                data[i % 6][3] = 'â˜”ë¹„'
            elif item['fcstValue'] == '2':
                data[i % 6][3] = 'ğŸŒ§ë¹„/ëˆˆ'
            elif item['fcstValue'] == '5':
                data[i % 6][3] = 'ğŸ’¦ë¹—ë°©ìš¸'
            elif item['fcstValue'] == '6':
                data[i % 6][3] = 'ğŸ’¦ë¹—ë°©ìš¸ëˆˆë‚ ë¦¼'
            elif item['fcstValue'] == '7':
                data[i % 6][3] = 'â„ï¸ëˆˆë‚ ë¦¼'
            else:
                data[i % 6][3] = 'âš ï¸API ì—ëŸ¬'


        # ê°•ìˆ˜ëŸ‰ (RN1)
        elif 12 <= i < 18:
            data[i % 6][4] = item['fcstValue']

        # í•˜ëŠ˜ ìƒíƒœ (SKY)
        # Add few more options
        elif 18 <= i < 24:
            if item['fcstValue'] == '1':
                data[i % 6][1] = 'â˜€ï¸'
                data[i % 6][2] = 'ë§‘ìŒ'
            elif item['fcstValue'] == '3':
                data[i % 6][1] = 'ğŸŒ¥'
                data[i % 6][2] = 'êµ¬ë¦„ë§ìŒ'
            elif item['fcstValue'] == '4':
                data[i % 6][1] = 'â˜ï¸'
                data[i % 6][2] = 'íë¦¼'
            else:
                data[i % 6][1] = 'âš ï¸'
                data[i % 6][2] = 'API ì—ëŸ¬'

        # ê¸°ì˜¨ (TH1)
        elif 24 <= i < 30:
            data[i % 6][0] = item['fcstValue']

        # ìŠµë„ (REH)
        elif 30 <= i < 36:
            data[i % 6][5] = item['fcstValue']

        # í’í–¥ (VEC)
        elif 48 <= i < 54:
            data[i % 6][6] = item['fcstValue']
            direction_num = int((int(item['fcstValue']) + 22.5 * 0.5) / 22.5)
            data[i % 6][7] = direction[direction_num]

        # í’ì† (WSD)
        elif 54 <= i < 60:
            data[i % 6][8] = item['fcstValue']
    return today, data
